{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75c231d8",
      "metadata": {
        "id": "75c231d8"
      },
      "source": [
        "\n",
        "# （CNN / ViT / T2T-ViT / EfficientNetV2）four models comparison on MNIST dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268c8808",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "268c8808",
        "outputId": "67ab9852-7a2e-4fdb-8c6c-4ea1c7fce744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# environment\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if device.type == 'cuda':\n",
        "    try:\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# seed\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b547601",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b547601",
        "outputId": "d04594b3-fb63-4b77-e29a-c56537171aa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 344kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.18MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.99MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(391, 79, 79)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "])\n",
        "\n",
        "trainval = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# split train/val\n",
        "train_size = 50000\n",
        "val_size = len(trainval) - train_size\n",
        "trainset, valset = random_split(trainval, [train_size, val_size], generator=torch.Generator().manual_seed(123))\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=(device.type=='cuda'))\n",
        "val_loader   = DataLoader(valset,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=(device.type=='cuda'))\n",
        "test_loader  = DataLoader(testset,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=(device.type=='cuda'))\n",
        "\n",
        "len(train_loader), len(val_loader), len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63dbc9a2",
      "metadata": {
        "id": "63dbc9a2"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = crit(logits, y)\n",
        "        loss_sum += loss.item()\n",
        "        pred = logits.argmax(1)\n",
        "        total += y.size(0)\n",
        "        correct += (pred == y).sum().item()\n",
        "    return loss_sum / len(loader), 100.0 * correct / total\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, wd=1e-4, name=\"model\"):\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sch = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "\n",
        "    hist = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"times\": [],  \n",
        "    }\n",
        "    start0 = time.time()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total, correct, run_loss = 0, 0, 0.0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            run_loss += loss.item()\n",
        "            pred = logits.argmax(1)\n",
        "            total += y.size(0)\n",
        "            correct += (pred == y).sum().item()\n",
        "\n",
        "        train_loss = run_loss / len(train_loader)\n",
        "        train_acc  = 100.0 * correct / total\n",
        "        _, val_acc = evaluate(model, val_loader)\n",
        "        _, test_acc = evaluate(model, test_loader)\n",
        "\n",
        "        hist[\"train_loss\"].append(train_loss)\n",
        "        hist[\"train_acc\"].append(train_acc)\n",
        "        hist[\"val_acc\"].append(val_acc)\n",
        "        hist[\"test_acc\"].append(test_acc)\n",
        "        hist[\"times\"].append(time.time() - start0)\n",
        "\n",
        "        sch.step()\n",
        "        print(f\"[{name}] Epoch {ep+1:02d}: loss={train_loss:.4f}  train={train_acc:.2f}%  val={val_acc:.2f}%  test={test_acc:.2f}%\")\n",
        "\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e63be3a",
      "metadata": {
        "id": "0e63be3a"
      },
      "outputs": [],
      "source": [
        "# --- model 1：Simple CNN ---\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2), nn.Dropout(0.25),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# --- model 2：ViT ---\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=28, patch_size=4, in_channels=1, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MSA(nn.Module):\n",
        "    def __init__(self, dim=64, heads=4, p=0.1):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "        self.h = heads\n",
        "        self.d = dim // heads\n",
        "        self.qkv = nn.Linear(dim, dim*3)\n",
        "        self.out = nn.Linear(dim, dim)\n",
        "        self.drop = nn.Dropout(p)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.h, self.d).permute(2,0,3,1,4)\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2,-1)) * (self.d ** -0.5)\n",
        "        attn = attn.softmax(-1)\n",
        "        attn = self.drop(attn)\n",
        "        out = (attn @ v).transpose(1,2).reshape(B,N,C)\n",
        "        return self.out(out)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim=64, ratio=4, p=0.1):\n",
        "        super().__init__()\n",
        "        hid = int(dim*ratio)\n",
        "        self.fc1 = nn.Linear(dim, hid)\n",
        "        self.fc2 = nn.Linear(hid, dim)\n",
        "        self.drop = nn.Dropout(p)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x); x = F.gelu(x); x = self.drop(x)\n",
        "        x = self.fc2(x); x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim=64, heads=4, ratio=4, p=0.1):\n",
        "        super().__init__()\n",
        "        self.n1 = nn.LayerNorm(dim)\n",
        "        self.attn = MSA(dim, heads, p)\n",
        "        self.n2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, ratio, p)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.n1(x))\n",
        "        x = x + self.mlp(self.n2(x))\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=28, patch=4, dim=64, layers=6, heads=4, n_classes=10, p=0.1):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Parameter(torch.zeros(1, (img_size//patch)**2 + 1, dim))\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,dim))\n",
        "        self.embed = PatchEmbedding(img_size, patch, 1, dim)\n",
        "        self.blocks = nn.ModuleList([Block(dim, heads, 4, p) for _ in range(layers)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, n_classes)\n",
        "        with torch.no_grad():\n",
        "            self.pe.normal_(std=0.02); self.cls.normal_(std=0.02)\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.embed(x)\n",
        "        cls = self.cls.expand(B, -1, -1)\n",
        "        x = torch.cat([cls, x], dim=1) + self.pe\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x[:,0])\n",
        "        return self.head(x)\n",
        "\n",
        "# --- model 3：T2T-ViT ---\n",
        "class T2TModule(nn.Module):\n",
        "    def __init__(self, in_ch=1, token_dim=64, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.stage1 = nn.Conv2d(in_ch, token_dim, 4, 4)\n",
        "        self.stage2 = nn.Conv2d(token_dim, embed_dim, 2, 2)\n",
        "        self.stage3 = nn.Conv2d(embed_dim, embed_dim, 3, 3)\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.stage1(x))\n",
        "        x = F.gelu(self.stage2(x))\n",
        "        x = F.gelu(self.stage3(x))\n",
        "        x = x.flatten(2).transpose(1,2)\n",
        "        return x\n",
        "\n",
        "class T2TViT(nn.Module):\n",
        "    def __init__(self, dim=64, heads=4, layers=6, n_classes=10):\n",
        "        super().__init__()\n",
        "        self.t2t = T2TModule(1, 64, dim)\n",
        "        self.pe = nn.Parameter(torch.zeros(1, 1 + 1*1, dim)) \n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,dim))\n",
        "        self.blocks = nn.ModuleList([Block(dim, heads) for _ in range(layers)])\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.head = nn.Linear(dim, n_classes)\n",
        "        with torch.no_grad():\n",
        "            self.pe.normal_(std=0.02); self.cls.normal_(std=0.02)\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.t2t(x)           \n",
        "        cls = self.cls.expand(B, -1, -1)\n",
        "        pe = F.interpolate(self.pe.transpose(1,2), size=(x.size(1),), mode='linear', align_corners=False).transpose(1,2)\n",
        "        x = torch.cat([cls, x], dim=1) + pe\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x[:,0])\n",
        "        return self.head(x)\n",
        "\n",
        "# --- model 4：EfficientNetV2 ---\n",
        "def try_build_efficientnetv2(num_classes=10):\n",
        "    try:\n",
        "        import timm\n",
        "        model = timm.create_model('efficientnetv2_s', pretrained=False, num_classes=num_classes, in_chans=1)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"timm/efficientnetv2 不可用，改用 Tiny 近似实现。原因：\", e)\n",
        "\n",
        "        # Fused-MBConv like tiny\n",
        "        def conv_bn_act(in_c, out_c, k=3, s=1):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, k, s, k//2, bias=False),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.SiLU()\n",
        "            )\n",
        "        class FusedMBConv(nn.Module):\n",
        "            def __init__(self, in_c, out_c, expand=4, k=3, s=1):\n",
        "                super().__init__()\n",
        "                mid = int(in_c*expand)\n",
        "                self.block = nn.Sequential(\n",
        "                    conv_bn_act(in_c, mid, k, s),\n",
        "                    conv_bn_act(mid, out_c, 1, 1)\n",
        "                )\n",
        "                self.use_skip = (s==1 and in_c==out_c)\n",
        "            def forward(self, x):\n",
        "                out = self.block(x)\n",
        "                return x + out if self.use_skip else out\n",
        "\n",
        "        class EfficientNetV2Tiny(nn.Module):\n",
        "            def __init__(self, num_classes=10):\n",
        "                super().__init__()\n",
        "                self.stem = conv_bn_act(1, 24, 3, 1)\n",
        "                self.stage1 = FusedMBConv(24, 24, expand=2, k=3, s=1)\n",
        "                self.stage2 = FusedMBConv(24, 48, expand=4, k=3, s=2)\n",
        "                self.stage3 = FusedMBConv(48, 64, expand=4, k=3, s=2)\n",
        "                self.stage4 = FusedMBConv(64, 96, expand=4, k=3, s=2)\n",
        "                self.head = nn.Sequential(\n",
        "                    nn.AdaptiveAvgPool2d(1),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(96, num_classes)\n",
        "                )\n",
        "            def forward(self, x):\n",
        "                x = self.stem(x)\n",
        "                x = self.stage1(x)\n",
        "                x = self.stage2(x)\n",
        "                x = self.stage3(x)\n",
        "                x = self.stage4(x)\n",
        "                x = self.head(x)\n",
        "                return x\n",
        "\n",
        "        return EfficientNetV2Tiny(num_classes)\n",
        "\n",
        "# Build model by name\n",
        "def build_model(name):\n",
        "    name = name.lower()\n",
        "    if name == \"cnn\":\n",
        "        return SimpleCNN().to(device)\n",
        "    if name == \"vit\":\n",
        "        return ViT().to(device)\n",
        "    if name == \"t2t-vit\":\n",
        "        return T2TViT().to(device)\n",
        "    if name == \"efficientnetv2\":\n",
        "        return try_build_efficientnetv2().to(device)\n",
        "    raise ValueError(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21199617",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21199617",
        "outputId": "0e29c381-2f32-4a12-db51-937971fc88b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== 训练 CNN ====\n",
            "[CNN] Epoch 01: loss=0.2620  train=91.59%  val=98.10%  test=98.51%\n",
            "[CNN] Epoch 02: loss=0.0801  train=97.59%  val=98.66%  test=98.87%\n",
            "[CNN] Epoch 03: loss=0.0602  train=98.15%  val=98.73%  test=99.00%\n"
          ]
        }
      ],
      "source": [
        "epochs = 20  \n",
        "lr = 1e-3\n",
        "wd = 1e-4\n",
        "\n",
        "histories = {}\n",
        "for name in [\"CNN\", \"ViT\", \"T2T-ViT\", \"EfficientNetV2\"]:\n",
        "    print(\"\\n==== 训练\", name, \"====\")\n",
        "    model = build_model(name)\n",
        "    h = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, wd=wd, name=name)\n",
        "    histories[name] = h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b14413",
      "metadata": {
        "id": "57b14413"
      },
      "outputs": [],
      "source": [
        "# plotting\n",
        "def plot_metric(metric_key, ylabel, title):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    for name, h in histories.items():\n",
        "        plt.plot(h[metric_key], label=name)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 1) Train Loss\n",
        "plot_metric(\"train_loss\", \"Loss\", \"Loss comparison\")\n",
        "\n",
        "# 2) Train Acc\n",
        "plot_metric(\"train_acc\", \"Accuracy (%)\", \"train accuracy\")\n",
        "\n",
        "# 3) Test Acc\n",
        "plot_metric(\"test_acc\", \"Accuracy (%)\", \"test accuracy\")\n",
        "\n",
        "# 4) Test Acc vs. Time\n",
        "plt.figure(figsize=(6,4))\n",
        "for name, h in histories.items():\n",
        "    plt.plot(h[\"times\"], h[\"test_acc\"], marker='o', label=name)\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Test Accuracy (%)\")\n",
        "plt.title(\"accuracy-time comparison\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
